{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Visualization\n",
    "\n",
    "This notebook visualizes attention patterns and compares standard vs linear attention.\n",
    "\n",
    "## Learning Objectives\n",
    "- Visualize attention weight distributions\n",
    "- Compare standard (softmax) vs linear attention patterns\n",
    "- Understand how causal masking affects attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaled Dot-Product Attention\n",
    "\n",
    "First, let's implement and visualize standard attention:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(query, key, value, causal=False, return_weights=True):\n",
    "    \"\"\"\n",
    "    Standard scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch, heads, seq_len, head_dim)\n",
    "        key: (batch, heads, seq_len, head_dim)\n",
    "        value: (batch, heads, seq_len, head_dim)\n",
    "        causal: Whether to apply causal masking\n",
    "        return_weights: Whether to return attention weights\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, heads, seq_len, head_dim)\n",
    "        attention_weights: (batch, heads, seq_len, seq_len) if return_weights\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    seq_len = query.size(-2)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    \n",
    "    # Apply causal mask if needed\n",
    "    if causal:\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=query.device), diagonal=1)\n",
    "        scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    if return_weights:\n",
    "        return output, attention_weights\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Attention\n",
    "\n",
    "Linear attention uses a feature map and reorders the computation:\n",
    "\n",
    "$$\\text{LinearAttention}(Q, K, V) = \\frac{\\phi(Q)(\\phi(K)^T V)}{\\phi(Q)\\phi(K)^T \\mathbf{1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_plus_one(x):\n",
    "    \"\"\"ELU + 1 feature map for linear attention.\"\"\"\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "def linear_attention(query, key, value, causal=False, return_weights=True, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Linear attention with ELU+1 feature map.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch, heads, seq_len, head_dim)\n",
    "        key: (batch, heads, seq_len, head_dim)\n",
    "        value: (batch, heads, seq_len, head_dim)\n",
    "        causal: Whether to apply causal masking\n",
    "        return_weights: Whether to compute attention weights (expensive!)\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, heads, seq_len, head_dim)\n",
    "        attention_weights: (batch, heads, seq_len, seq_len) if return_weights\n",
    "    \"\"\"\n",
    "    # Apply feature map\n",
    "    q = elu_plus_one(query)\n",
    "    k = elu_plus_one(key)\n",
    "    \n",
    "    if not causal:\n",
    "        # Bidirectional: KV = K^T @ V, then Q @ KV\n",
    "        kv = torch.einsum('bhnd,bhnv->bhdv', k, value)  # (B, H, D, D)\n",
    "        k_sum = k.sum(dim=2)  # (B, H, D)\n",
    "        \n",
    "        output = torch.einsum('bhnd,bhdv->bhnv', q, kv)\n",
    "        normalizer = torch.einsum('bhnd,bhd->bhn', q, k_sum).unsqueeze(-1)\n",
    "        output = output / (normalizer + eps)\n",
    "    else:\n",
    "        # Causal: cumulative sum\n",
    "        kv = torch.einsum('bhnd,bhnv->bhndv', k, value)\n",
    "        kv_cumsum = torch.cumsum(kv, dim=2)\n",
    "        k_cumsum = torch.cumsum(k, dim=2)\n",
    "        \n",
    "        output = torch.einsum('bhnd,bhndv->bhnv', q, kv_cumsum)\n",
    "        normalizer = torch.einsum('bhnd,bhnd->bhn', q, k_cumsum).unsqueeze(-1)\n",
    "        output = output / (normalizer + eps)\n",
    "    \n",
    "    if return_weights:\n",
    "        # Compute \"attention weights\" for visualization\n",
    "        # Note: These aren't used in the actual computation\n",
    "        scores = torch.einsum('bhnd,bhmd->bhnm', q, k)\n",
    "        if causal:\n",
    "            seq_len = query.size(-2)\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=query.device), diagonal=1)\n",
    "            scores = scores.masked_fill(mask.bool(), 0)\n",
    "        weights = scores / (scores.sum(dim=-1, keepdim=True) + eps)\n",
    "        return output, weights\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention Patterns\n",
    "\n",
    "Let's create some sample data and visualize the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "num_heads = 1\n",
    "seq_len = 32\n",
    "head_dim = 16\n",
    "\n",
    "# Random queries, keys, values\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(f\"Q, K, V shapes: {q.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention for both methods\n",
    "_, standard_weights = standard_attention(q, k, v, causal=False)\n",
    "_, linear_weights = linear_attention(q, k, v, causal=False)\n",
    "\n",
    "# Also compute causal versions\n",
    "_, standard_causal_weights = standard_attention(q, k, v, causal=True)\n",
    "_, linear_causal_weights = linear_attention(q, k, v, causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(weights, title, ax):\n",
    "    \"\"\"Plot attention weights as a heatmap.\"\"\"\n",
    "    w = weights[0, 0].detach().numpy()\n",
    "    im = ax.imshow(w, cmap='Blues', aspect='auto')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    return im\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "plot_attention(standard_weights, 'Standard Attention (Bidirectional)', axes[0, 0])\n",
    "plot_attention(linear_weights, 'Linear Attention (Bidirectional)', axes[0, 1])\n",
    "plot_attention(standard_causal_weights, 'Standard Attention (Causal)', axes[1, 0])\n",
    "plot_attention(linear_causal_weights, 'Linear Attention (Causal)', axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/attention_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Distribution Comparison\n",
    "\n",
    "Let's look at how the attention distributions differ for a single query position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for a single query\n",
    "query_pos = 15\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bidirectional\n",
    "axes[0].bar(range(seq_len), standard_weights[0, 0, query_pos].detach().numpy(), \n",
    "            alpha=0.7, label='Standard (Softmax)')\n",
    "axes[0].bar(range(seq_len), linear_weights[0, 0, query_pos].detach().numpy(), \n",
    "            alpha=0.7, label='Linear (ELU+1)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Attention Weight')\n",
    "axes[0].set_title(f'Attention Distribution for Query Position {query_pos} (Bidirectional)')\n",
    "axes[0].legend()\n",
    "axes[0].axvline(x=query_pos, color='red', linestyle='--', alpha=0.5, label='Query Position')\n",
    "\n",
    "# Causal\n",
    "axes[1].bar(range(seq_len), standard_causal_weights[0, 0, query_pos].detach().numpy(), \n",
    "            alpha=0.7, label='Standard (Softmax)')\n",
    "axes[1].bar(range(seq_len), linear_causal_weights[0, 0, query_pos].detach().numpy(), \n",
    "            alpha=0.7, label='Linear (ELU+1)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Attention Weight')\n",
    "axes[1].set_title(f'Attention Distribution for Query Position {query_pos} (Causal)')\n",
    "axes[1].legend()\n",
    "axes[1].axvline(x=query_pos, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/attention_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "1. **Softmax attention is \"peakier\"**: Standard attention tends to concentrate on a few key positions\n",
    "2. **Linear attention is smoother**: The ELU+1 feature map produces more diffuse attention\n",
    "3. **Causal masking works in both**: The lower triangular pattern is preserved\n",
    "\n",
    "This difference in attention patterns explains why linear attention may not be a drop-in replacement for all tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Analysis\n",
    "\n",
    "Let's quantify how \"spread out\" the attention is using entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_entropy(weights, eps=1e-10):\n",
    "    \"\"\"Compute entropy of attention distributions.\"\"\"\n",
    "    # Clamp to avoid log(0)\n",
    "    w = weights.clamp(min=eps)\n",
    "    entropy = -torch.sum(w * torch.log(w), dim=-1)\n",
    "    return entropy\n",
    "\n",
    "# Compute entropy\n",
    "standard_entropy = attention_entropy(standard_weights).mean().item()\n",
    "linear_entropy = attention_entropy(linear_weights).mean().item()\n",
    "max_entropy = np.log(seq_len)  # Uniform distribution\n",
    "\n",
    "print(f\"Standard Attention Entropy: {standard_entropy:.3f}\")\n",
    "print(f\"Linear Attention Entropy: {linear_entropy:.3f}\")\n",
    "print(f\"Maximum Possible Entropy (uniform): {max_entropy:.3f}\")\n",
    "print(f\"\\nLinear attention is {linear_entropy/standard_entropy:.1f}x more spread out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [02_complexity_analysis.ipynb](02_complexity_analysis.ipynb): Analyze computational complexity\n",
    "- [03_memory_profiling.ipynb](03_memory_profiling.ipynb): Profile memory usage\n",
    "- [04_benchmark_comparison.ipynb](04_benchmark_comparison.ipynb): Full benchmark comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
