{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Analysis\n",
    "\n",
    "This notebook empirically verifies the O(n²) vs O(n) complexity of standard vs linear attention.\n",
    "\n",
    "## Learning Objectives\n",
    "- Measure actual compute time at different sequence lengths\n",
    "- Verify theoretical complexity predictions\n",
    "- Identify the crossover point where linear attention becomes faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "\n",
    "# Check for CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention_forward(q, k, v, causal=False):\n",
    "    \"\"\"Standard scaled dot-product attention.\"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    \n",
    "    if causal:\n",
    "        seq_len = q.size(-2)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=q.device), diagonal=1)\n",
    "        scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
    "    \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, v)\n",
    "\n",
    "\n",
    "def linear_attention_forward(q, k, v, causal=False, eps=1e-6):\n",
    "    \"\"\"Linear attention with ELU+1 feature map.\"\"\"\n",
    "    # Feature map\n",
    "    q = F.elu(q) + 1\n",
    "    k = F.elu(k) + 1\n",
    "    \n",
    "    if not causal:\n",
    "        # Bidirectional: O(nd²)\n",
    "        kv = torch.einsum('bhnd,bhnv->bhdv', k, v)\n",
    "        k_sum = k.sum(dim=2)\n",
    "        out = torch.einsum('bhnd,bhdv->bhnv', q, kv)\n",
    "        norm = torch.einsum('bhnd,bhd->bhn', q, k_sum).unsqueeze(-1)\n",
    "        return out / (norm + eps)\n",
    "    else:\n",
    "        # Causal: O(nd²) but with cumsum\n",
    "        kv = torch.einsum('bhnd,bhnv->bhndv', k, v)\n",
    "        kv_cumsum = torch.cumsum(kv, dim=2)\n",
    "        k_cumsum = torch.cumsum(k, dim=2)\n",
    "        out = torch.einsum('bhnd,bhndv->bhnv', q, kv_cumsum)\n",
    "        norm = torch.einsum('bhnd,bhnd->bhn', q, k_cumsum).unsqueeze(-1)\n",
    "        return out / (norm + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(fn, q, k, v, causal=False, warmup=3, iters=10):\n",
    "    \"\"\"\n",
    "    Benchmark an attention function.\n",
    "    \n",
    "    Returns:\n",
    "        mean_time: Average time in milliseconds\n",
    "        std_time: Standard deviation\n",
    "    \"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = fn(q, k, v, causal)\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "        else:\n",
    "            start_time = perf_counter()\n",
    "        \n",
    "        _ = fn(q, k, v, causal)\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(start.elapsed_time(end))\n",
    "        else:\n",
    "            times.append((perf_counter() - start_time) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity Scaling Experiment\n",
    "\n",
    "Let's measure how runtime scales with sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 4\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "\n",
    "# Sequence lengths to test\n",
    "seq_lengths = [256, 512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "# Add longer sequences if we have enough memory\n",
    "if device == 'cuda':\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    if gpu_memory > 20e9:  # More than 20GB\n",
    "        seq_lengths.extend([16384])\n",
    "    if gpu_memory > 40e9:  # More than 40GB\n",
    "        seq_lengths.extend([32768])\n",
    "\n",
    "print(f\"Testing sequence lengths: {seq_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\nBenchmarking seq_len={seq_len}...\")\n",
    "    \n",
    "    # Create test data\n",
    "    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)\n",
    "    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)\n",
    "    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)\n",
    "    \n",
    "    try:\n",
    "        # Standard attention (bidirectional)\n",
    "        std_mean, std_std = benchmark_attention(standard_attention_forward, q, k, v, causal=False)\n",
    "        print(f\"  Standard (bidir): {std_mean:.2f} +/- {std_std:.2f} ms\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  Standard (bidir): OOM\")\n",
    "        std_mean, std_std = float('nan'), float('nan')\n",
    "    \n",
    "    try:\n",
    "        # Standard attention (causal)\n",
    "        std_causal_mean, std_causal_std = benchmark_attention(standard_attention_forward, q, k, v, causal=True)\n",
    "        print(f\"  Standard (causal): {std_causal_mean:.2f} +/- {std_causal_std:.2f} ms\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  Standard (causal): OOM\")\n",
    "        std_causal_mean, std_causal_std = float('nan'), float('nan')\n",
    "    \n",
    "    # Linear attention (bidirectional)\n",
    "    lin_mean, lin_std = benchmark_attention(linear_attention_forward, q, k, v, causal=False)\n",
    "    print(f\"  Linear (bidir): {lin_mean:.2f} +/- {lin_std:.2f} ms\")\n",
    "    \n",
    "    # Linear attention (causal)\n",
    "    lin_causal_mean, lin_causal_std = benchmark_attention(linear_attention_forward, q, k, v, causal=True)\n",
    "    print(f\"  Linear (causal): {lin_causal_mean:.2f} +/- {lin_causal_std:.2f} ms\")\n",
    "    \n",
    "    results.append({\n",
    "        'seq_len': seq_len,\n",
    "        'standard_bidir': std_mean,\n",
    "        'standard_causal': std_causal_mean,\n",
    "        'linear_bidir': lin_mean,\n",
    "        'linear_causal': lin_causal_mean,\n",
    "    })\n",
    "    \n",
    "    # Clean up\n",
    "    del q, k, v\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bidirectional comparison\n",
    "ax = axes[0]\n",
    "ax.plot(df['seq_len'], df['standard_bidir'], 'o-', label='Standard Attention', linewidth=2)\n",
    "ax.plot(df['seq_len'], df['linear_bidir'], 's-', label='Linear Attention', linewidth=2)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "ax.set_title('Bidirectional Attention Scaling')\n",
    "ax.legend()\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Causal comparison\n",
    "ax = axes[1]\n",
    "ax.plot(df['seq_len'], df['standard_causal'], 'o-', label='Standard Attention', linewidth=2)\n",
    "ax.plot(df['seq_len'], df['linear_causal'], 's-', label='Linear Attention', linewidth=2)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "ax.set_title('Causal Attention Scaling')\n",
    "ax.legend()\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/complexity_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute speedups\n",
    "df['speedup_bidir'] = df['standard_bidir'] / df['linear_bidir']\n",
    "df['speedup_causal'] = df['standard_causal'] / df['linear_causal']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df['speedup_bidir'], width, label='Bidirectional', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, df['speedup_causal'], width, label='Causal', color='coral')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Break-even')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Speedup (Standard / Linear)')\n",
    "ax.set_title('Linear Attention Speedup vs Sequence Length')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['seq_len'].astype(int))\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if not np.isnan(height):\n",
    "        ax.annotate(f'{height:.1f}x',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/speedup_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical vs Empirical Scaling\n",
    "\n",
    "Let's verify that the scaling matches theory:\n",
    "- Standard attention: O(n²)\n",
    "- Linear attention: O(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit curves to verify complexity\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def quadratic(n, a, b):\n",
    "    return a * n**2 + b\n",
    "\n",
    "def linear(n, a, b):\n",
    "    return a * n + b\n",
    "\n",
    "# Filter out NaN values\n",
    "valid_mask = ~df['standard_bidir'].isna()\n",
    "seq_valid = df.loc[valid_mask, 'seq_len'].values\n",
    "std_valid = df.loc[valid_mask, 'standard_bidir'].values\n",
    "lin_valid = df.loc[valid_mask, 'linear_bidir'].values\n",
    "\n",
    "try:\n",
    "    # Fit standard attention to quadratic\n",
    "    popt_std, _ = curve_fit(quadratic, seq_valid, std_valid, p0=[1e-6, 0])\n",
    "    print(f\"Standard attention fit: {popt_std[0]:.2e} * n² + {popt_std[1]:.2f}\")\n",
    "    \n",
    "    # Fit linear attention to linear\n",
    "    popt_lin, _ = curve_fit(linear, seq_valid, lin_valid, p0=[1e-3, 0])\n",
    "    print(f\"Linear attention fit: {popt_lin[0]:.2e} * n + {popt_lin[1]:.2f}\")\n",
    "    \n",
    "    # Plot with fitted curves\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    n_plot = np.linspace(256, max(seq_valid) * 1.5, 100)\n",
    "    \n",
    "    ax.scatter(seq_valid, std_valid, s=100, label='Standard (measured)', marker='o')\n",
    "    ax.plot(n_plot, quadratic(n_plot, *popt_std), '--', label=f'O(n²) fit', alpha=0.7)\n",
    "    \n",
    "    ax.scatter(seq_valid, lin_valid, s=100, label='Linear (measured)', marker='s')\n",
    "    ax.plot(n_plot, linear(n_plot, *popt_lin), '--', label=f'O(n) fit', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Time (ms)')\n",
    "    ax.set_title('Theoretical vs Empirical Complexity')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, max(seq_valid) * 1.2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/theoretical_vs_empirical.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Curve fitting failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Standard attention scales quadratically** with sequence length\n",
    "2. **Linear attention scales linearly** with sequence length\n",
    "3. **Crossover point** around 4K-8K sequence length (varies by hardware)\n",
    "4. **Bidirectional shows larger speedups** than causal\n",
    "\n",
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "df.to_csv('../results/benchmarks/complexity_analysis.csv', index=False)\n",
    "print(\"Results saved to ../results/benchmarks/complexity_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [03_memory_profiling.ipynb](03_memory_profiling.ipynb): Analyze memory usage\n",
    "- [04_benchmark_comparison.ipynb](04_benchmark_comparison.ipynb): Compare with optimized implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
