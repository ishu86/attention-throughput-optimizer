{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Profiling\n",
    "\n",
    "This notebook profiles memory usage of different attention implementations.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand memory consumption of attention mechanisms\n",
    "- Use the ATO profiling tools\n",
    "- Compare memory scaling of standard vs linear attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Check for CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    print(f\"Total Memory: {total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Tracking Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTracker:\n",
    "    \"\"\"Simple GPU memory tracker.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def current_allocated(self):\n",
    "        if device == 'cuda':\n",
    "            return torch.cuda.memory_allocated() / 1e6  # MB\n",
    "        return 0\n",
    "    \n",
    "    def peak_allocated(self):\n",
    "        if device == 'cuda':\n",
    "            return torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "        return 0\n",
    "    \n",
    "    def reserved(self):\n",
    "        if device == 'cuda':\n",
    "            return torch.cuda.memory_reserved() / 1e6  # MB\n",
    "        return 0\n",
    "\n",
    "tracker = MemoryTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure memory usage of a function.\n",
    "    \n",
    "    Returns:\n",
    "        peak_memory: Peak memory usage in MB\n",
    "    \"\"\"\n",
    "    tracker.reset()\n",
    "    \n",
    "    # Get baseline memory\n",
    "    baseline = tracker.current_allocated()\n",
    "    \n",
    "    # Run function\n",
    "    result = fn(*args, **kwargs)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Get peak memory\n",
    "    peak = tracker.peak_allocated()\n",
    "    \n",
    "    return peak - baseline, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(q, k, v):\n",
    "    \"\"\"Standard attention - materializes n×n attention matrix.\"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, v)\n",
    "\n",
    "\n",
    "def linear_attention_bidir(q, k, v, eps=1e-6):\n",
    "    \"\"\"Linear attention - O(d²) intermediate state.\"\"\"\n",
    "    q = F.elu(q) + 1\n",
    "    k = F.elu(k) + 1\n",
    "    \n",
    "    kv = torch.einsum('bhnd,bhnv->bhdv', k, v)  # (B, H, D, D)\n",
    "    k_sum = k.sum(dim=2)\n",
    "    \n",
    "    out = torch.einsum('bhnd,bhdv->bhnv', q, kv)\n",
    "    norm = torch.einsum('bhnd,bhd->bhn', q, k_sum).unsqueeze(-1)\n",
    "    return out / (norm + eps)\n",
    "\n",
    "\n",
    "def linear_attention_causal(q, k, v, eps=1e-6):\n",
    "    \"\"\"Causal linear attention - uses cumsum.\"\"\"\n",
    "    q = F.elu(q) + 1\n",
    "    k = F.elu(k) + 1\n",
    "    \n",
    "    kv = torch.einsum('bhnd,bhnv->bhndv', k, v)  # (B, H, N, D, D)\n",
    "    kv_cumsum = torch.cumsum(kv, dim=2)\n",
    "    k_cumsum = torch.cumsum(k, dim=2)\n",
    "    \n",
    "    out = torch.einsum('bhnd,bhndv->bhnv', q, kv_cumsum)\n",
    "    norm = torch.einsum('bhnd,bhnd->bhn', q, k_cumsum).unsqueeze(-1)\n",
    "    return out / (norm + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Memory Analysis\n",
    "\n",
    "Before measuring, let's compute theoretical memory requirements:\n",
    "\n",
    "### Standard Attention\n",
    "- Attention matrix: `B × H × N × N × sizeof(dtype)`\n",
    "- At N=8192, B=4, H=8, fp16: 4 × 8 × 8192² × 2 = **4.3 GB** just for attention scores!\n",
    "\n",
    "### Linear Attention (Bidirectional)\n",
    "- KV state: `B × H × D × D × sizeof(dtype)`\n",
    "- At D=64, B=4, H=8, fp16: 4 × 8 × 64 × 64 × 2 = **2 MB** regardless of N\n",
    "\n",
    "### Linear Attention (Causal)\n",
    "- Cumulative KV: `B × H × N × D × D × sizeof(dtype)`\n",
    "- At N=8192, D=64, B=4, H=8, fp16: 4 × 8 × 8192 × 64 × 64 × 2 = **17 GB** (worse than standard!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_memory(batch_size, num_heads, seq_len, head_dim, dtype='fp16'):\n",
    "    \"\"\"\n",
    "    Compute theoretical memory requirements.\n",
    "    \n",
    "    Returns dict with memory in MB for each attention type.\n",
    "    \"\"\"\n",
    "    bytes_per_elem = 2 if dtype == 'fp16' else 4\n",
    "    B, H, N, D = batch_size, num_heads, seq_len, head_dim\n",
    "    \n",
    "    # Input memory (Q, K, V)\n",
    "    input_mem = 3 * B * H * N * D * bytes_per_elem / 1e6\n",
    "    \n",
    "    # Standard attention: attention matrix\n",
    "    standard_attn_matrix = B * H * N * N * bytes_per_elem / 1e6\n",
    "    \n",
    "    # Linear bidir: KV state\n",
    "    linear_bidir_state = B * H * D * D * bytes_per_elem / 1e6\n",
    "    \n",
    "    # Linear causal: cumulative KV (N copies of state)\n",
    "    linear_causal_state = B * H * N * D * D * bytes_per_elem / 1e6\n",
    "    \n",
    "    return {\n",
    "        'inputs': input_mem,\n",
    "        'standard_peak': input_mem + standard_attn_matrix,\n",
    "        'linear_bidir_peak': input_mem + linear_bidir_state,\n",
    "        'linear_causal_peak': input_mem + linear_causal_state,\n",
    "    }\n",
    "\n",
    "# Example calculation\n",
    "theory = theoretical_memory(4, 8, 8192, 64)\n",
    "print(\"Theoretical memory at seq_len=8192:\")\n",
    "for key, val in theory.items():\n",
    "    print(f\"  {key}: {val:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Memory Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 4\n",
    "num_heads = 8\n",
    "head_dim = 64\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096]\n",
    "\n",
    "# Extend if we have enough memory\n",
    "if device == 'cuda' and total_memory > 20e9:\n",
    "    seq_lengths.append(8192)\n",
    "\n",
    "print(f\"Testing sequence lengths: {seq_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run memory measurements\n",
    "results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\nMeasuring seq_len={seq_len}...\")\n",
    "    \n",
    "    # Create inputs\n",
    "    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n",
    "    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n",
    "    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n",
    "    \n",
    "    input_mem = 3 * q.numel() * 2 / 1e6  # 3 tensors, 2 bytes per element\n",
    "    \n",
    "    row = {\n",
    "        'seq_len': seq_len,\n",
    "        'input_memory_mb': input_mem,\n",
    "    }\n",
    "    \n",
    "    # Standard attention\n",
    "    try:\n",
    "        mem, _ = measure_memory(standard_attention, q, k, v)\n",
    "        row['standard_memory_mb'] = mem\n",
    "        print(f\"  Standard: {mem:.1f} MB\")\n",
    "    except RuntimeError:\n",
    "        row['standard_memory_mb'] = float('nan')\n",
    "        print(f\"  Standard: OOM\")\n",
    "    \n",
    "    # Linear bidir\n",
    "    try:\n",
    "        mem, _ = measure_memory(linear_attention_bidir, q, k, v)\n",
    "        row['linear_bidir_memory_mb'] = mem\n",
    "        print(f\"  Linear (bidir): {mem:.1f} MB\")\n",
    "    except RuntimeError:\n",
    "        row['linear_bidir_memory_mb'] = float('nan')\n",
    "        print(f\"  Linear (bidir): OOM\")\n",
    "    \n",
    "    # Linear causal\n",
    "    try:\n",
    "        mem, _ = measure_memory(linear_attention_causal, q, k, v)\n",
    "        row['linear_causal_memory_mb'] = mem\n",
    "        print(f\"  Linear (causal): {mem:.1f} MB\")\n",
    "    except RuntimeError:\n",
    "        row['linear_causal_memory_mb'] = float('nan')\n",
    "        print(f\"  Linear (causal): OOM\")\n",
    "    \n",
    "    # Theoretical values\n",
    "    theory = theoretical_memory(batch_size, num_heads, seq_len, head_dim)\n",
    "    row['standard_theoretical_mb'] = theory['standard_peak']\n",
    "    row['linear_bidir_theoretical_mb'] = theory['linear_bidir_peak']\n",
    "    row['linear_causal_theoretical_mb'] = theory['linear_causal_peak']\n",
    "    \n",
    "    results.append(row)\n",
    "    \n",
    "    # Cleanup\n",
    "    del q, k, v\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Memory Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Empirical memory\n",
    "ax = axes[0]\n",
    "ax.plot(df['seq_len'], df['standard_memory_mb'], 'o-', label='Standard', linewidth=2)\n",
    "ax.plot(df['seq_len'], df['linear_bidir_memory_mb'], 's-', label='Linear (bidir)', linewidth=2)\n",
    "ax.plot(df['seq_len'], df['linear_causal_memory_mb'], '^-', label='Linear (causal)', linewidth=2)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Peak Memory (MB)')\n",
    "ax.set_title('Empirical Memory Usage')\n",
    "ax.legend()\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Theoretical vs empirical for standard\n",
    "ax = axes[1]\n",
    "valid = ~df['standard_memory_mb'].isna()\n",
    "ax.scatter(df.loc[valid, 'seq_len'], df.loc[valid, 'standard_memory_mb'], \n",
    "           s=100, label='Standard (measured)')\n",
    "ax.plot(df['seq_len'], df['standard_theoretical_mb'], '--', label='Standard (theoretical)')\n",
    "ax.scatter(df['seq_len'], df['linear_bidir_memory_mb'], \n",
    "           s=100, marker='s', label='Linear bidir (measured)')\n",
    "ax.plot(df['seq_len'], df['linear_bidir_theoretical_mb'], '--', label='Linear bidir (theoretical)')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Peak Memory (MB)')\n",
    "ax.set_title('Theoretical vs Empirical Memory')\n",
    "ax.legend()\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/memory_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Savings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute memory savings\n",
    "df['savings_bidir'] = df['standard_memory_mb'] / df['linear_bidir_memory_mb']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "valid = ~df['savings_bidir'].isna()\n",
    "ax.bar(range(len(df[valid])), df.loc[valid, 'savings_bidir'], color='steelblue')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Memory Savings (Standard / Linear)')\n",
    "ax.set_title('Linear Attention Memory Savings (Bidirectional)')\n",
    "ax.set_xticks(range(len(df[valid])))\n",
    "ax.set_xticklabels(df.loc[valid, 'seq_len'].astype(int))\n",
    "\n",
    "for i, (idx, row) in enumerate(df[valid].iterrows()):\n",
    "    ax.annotate(f'{row[\"savings_bidir\"]:.1f}x',\n",
    "                xy=(i, row['savings_bidir']),\n",
    "                xytext=(0, 5), textcoords=\"offset points\",\n",
    "                ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/memory_savings.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "1. **Standard attention memory scales O(n²)** - The attention matrix dominates\n",
    "2. **Linear bidirectional scales O(n)** - Only the KV state (O(d²)) is stored\n",
    "3. **Linear causal can be worse!** - The naive cumsum implementation stores O(n × d²)\n",
    "\n",
    "### The Causal Linear Attention Dilemma\n",
    "\n",
    "The naive causal implementation materializes the entire cumulative state:\n",
    "```python\n",
    "kv_cumsum = torch.cumsum(kv, dim=2)  # Shape: (B, H, N, D, D)\n",
    "```\n",
    "\n",
    "This is O(N × D²) which can exceed standard attention's O(N²) when D² > N!\n",
    "\n",
    "**Solutions:**\n",
    "1. Chunked processing (only keep current state)\n",
    "2. Recurrent implementation (sequential but memory-efficient)\n",
    "3. Parallel scan with state checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df.to_csv('../results/benchmarks/memory_profiling.csv', index=False)\n",
    "print(\"Results saved to ../results/benchmarks/memory_profiling.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [04_benchmark_comparison.ipynb](04_benchmark_comparison.ipynb): Full benchmark with optimized implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
